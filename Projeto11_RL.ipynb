{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luv4as/projeto11_RL/blob/main/Projeto11_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dyna-Q com Prioritized Sweeping"
      ],
      "metadata": {
        "id": "Um8QYYeAA3d_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instruções\n",
        "\n",
        "### Pergunta:\n",
        "- Qual o benefício de acrescentar “prioritized sweeping” (varredura priorizada) no Dyna-Q?\n",
        "\n",
        "### Detalhes:\n",
        "- Ver seção 8.4 do livro\n",
        "- Explicar, usando um ambiente grid, como o Dyna-Q pode fazer muitas atualizações “ruins” (no planejamento ou indirect RL)\n",
        "- Explicar a ideia de prioritized sweeping\n",
        "- Implementar uns dois esquemas de prioritized sweeping\n",
        "- Fazer experimentos com vários ambientes determinísticos\n",
        "- Avaliar em termos de:\n",
        " - “tempo” por steps\n",
        " - tempo real em máquina de 1 processador (wallclock time)\n",
        " - quantidade de updates\n",
        "- Experimentos (opções alternativas):\n",
        " - Comparar nos ambientes determinísticos (Taxi, Cliff, Racetrack, etc)\n",
        " - Labirintos de diferentes tamanhos, para reproduzir a figura abaixo (do livro)\n"
      ],
      "metadata": {
        "id": "jJuhfAW6ypDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports gerais"
      ],
      "metadata": {
        "id": "CCnvrZ9IxBzj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X62c3nS_C5YZ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    # for saving videos\n",
        "    !apt-get install ffmpeg\n",
        "\n",
        "    !pip install gymnasium moviepy\n",
        "    !pip install optuna\n",
        "\n",
        "    # clone repository\n",
        "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
        "    sys.path.append(\"/content/rl_facil\")\n",
        "\n",
        "else:\n",
        "    from os import path\n",
        "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random as rand\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from queue import PriorityQueue\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import optuna\n",
        "\n",
        "from util.experiments import repeated_exec\n",
        "from util.plot import plot_result, plot_multiple_results\n",
        "from util.notebook import display_videos_from_path\n",
        "from util.qtable_helper import evaluate_qtable_policy, record_video_qtable"
      ],
      "metadata": {
        "id": "X1STG8TYP1WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# basta importar o módulo que o ambiente \"RaceTrack-v0\" é registrado no gym\n",
        "import envs"
      ],
      "metadata": {
        "id": "QrdKjvk2P5Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dyna-Q\n"
      ],
      "metadata": {
        "id": "oA7eTttjP7iz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Por que o Dyna-Q pode fazer muitas atualizações \"ruins\"?\n",
        "- **Atualizações Uniformes:** O Dyna-Q realiza atualizações simuladas uniformemente para cada **estado** e **ação**, sem considerar a importância ou prioridade dessas atualizaçãoes.\n",
        "- **Ineficiente em Espaços Grandes**: Em espaços grandes, onde os estados e ações são numerosos (```CliffWalking```, por exemplo), o Dyna-Q pode se tornar ineficiente porque tenta simular e atualizar todos de forma uniforme, independentemente de sua relevância para o problema atual.\n",
        "- **Velocidade de Convergência**: Sem foco em atualizações mais significativas, o Dyna-Q pode demorar mais para alcançar uma política ótima ou quase ótima, especialmente em ambientes onde alguns estados têm um impacto muito maior nas recompensas cumulativas do que outros."
      ],
      "metadata": {
        "id": "4phTM4Q24HD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def planning(model, planning_steps, Q, lr, gamma):\n",
        "    all_s_a = list(model.keys())\n",
        "    if len(all_s_a) < planning_steps:\n",
        "        samples = rand.choices(all_s_a, k=planning_steps)\n",
        "    else:\n",
        "        samples = rand.sample(all_s_a, k=planning_steps)\n",
        "\n",
        "    for s, a in samples:\n",
        "        r, next_s, is_terminal = model[(s,a)]\n",
        "        if is_terminal:\n",
        "            V_next_s = 0\n",
        "        else:\n",
        "            V_next_s = np.max(Q[next_s])\n",
        "        delta = (r + gamma * V_next_s) - Q[s,a]\n",
        "        Q[s,a] = Q[s,a] + lr * delta"
      ],
      "metadata": {
        "id": "1AbyrMLoP-3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Esta é a política. Neste caso, escolhe uma ação com base nos valores da tabela Q, usando uma estratégia epsilon-greedy,\n",
        "# dividindo a probabilidade igualmente em caso de empates entre ações de valor máximo.\n",
        "from util.qtable_helper import epsilon_greedy"
      ],
      "metadata": {
        "id": "eFTJ-6QrQA0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution\n",
        "Os ambientes utilizados para treinamento são:\n",
        "- Taxi-v3\n",
        "- FrozenLake-v1\n",
        "- CliffWalking"
      ],
      "metadata": {
        "id": "pKLIOPGd4tOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Algoritmo Dyna Q\n",
        "def run_dyna_q(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1, planning_steps=5, verbose=False):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # inicializa a tabela Q\n",
        "    Q = np.random.uniform(low=-0.01, high=+0.01, size=(env.observation_space.n, num_actions))\n",
        "\n",
        "    # inicializa o modelo do ambiente\n",
        "    model = dict()\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "\n",
        "        state, _ = env.reset()\n",
        "\n",
        "        # executa 1 episódio completo, fazendo atualizações na Q-table\n",
        "        while not done:\n",
        "\n",
        "            # escolhe a próxima ação -- usa epsilon-greedy\n",
        "            action = epsilon_greedy(Q, state, epsilon)\n",
        "\n",
        "            # realiza a ação, ou seja, dá um passo no ambiente\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if terminated:\n",
        "                # para estados terminais\n",
        "                V_next_state = 0\n",
        "            else:\n",
        "                # para estados não-terminais -- valor máximo (melhor ação)\n",
        "                V_next_state = np.max(Q[next_state])\n",
        "\n",
        "            # atualiza a Q-table / direct RL\n",
        "            delta = (reward + gamma * V_next_state) - Q[state,action]\n",
        "            Q[state,action] = Q[state,action] + lr * delta\n",
        "\n",
        "            # atualiza o modelo\n",
        "            model[state,action] = (reward, next_state, terminated)\n",
        "\n",
        "            # planejamento / indirect RL\n",
        "            planning(model, planning_steps, Q, lr, gamma)\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "        sum_rewards_per_ep.append(sum_rewards)\n",
        "\n",
        "        # a cada 100 episódios, imprime informação sobre o progresso\n",
        "        if verbose and ((i+1) % 100 == 0):\n",
        "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
        "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "    state = env.reset()\n",
        "    reward = 0\n",
        "\n",
        "    return sum_rewards_per_ep, Q"
      ],
      "metadata": {
        "id": "-w7b5wc9QWzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# escolha o ambiente descomentando uma das linhas abaixo\n",
        "ENV_NAME = \"Taxi-v3\"\n",
        "#ENV_NAME = \"WindyGrid-v0\"\n",
        "#ENV_NAME = \"RaceTrack-v0\"\n",
        "\n",
        "LR = 0.3\n",
        "GAMMA = 0.90\n",
        "EPSILON = 0.1\n",
        "\n",
        "#VERBOSE = True"
      ],
      "metadata": {
        "id": "PEwVkBWeT82Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "rmax = 10.0\n",
        "EPISODES = 700\n",
        "\n",
        "LR = 0.3\n",
        "GAMMA = 0.90\n",
        "EPSILON = 0.1\n",
        "\n",
        "rewards2, qtable2 = run_dyna_q(env, EPISODES, LR, GAMMA, EPSILON, planning_steps=10, verbose=True)\n",
        "print(\"Últimos resultados: media =\", np.mean(rewards2[-20:]), \", desvio padrao =\", np.std(rewards2[-20:]))"
      ],
      "metadata": {
        "id": "Dls-a-jdQal2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostra um gráfico de passos x retornos não descontados acumulados\n",
        "plot_result(rewards2, rmax, cumulative='no', window=30)"
      ],
      "metadata": {
        "id": "Y9BcSu-GQbQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_qtable_policy(env, qtable2, 10, verbose=True);"
      ],
      "metadata": {
        "id": "DMH1GmS4QclH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "record_video_qtable(ENV_NAME, qtable2, episodes=3, folder='./videos-dynaq')"
      ],
      "metadata": {
        "id": "ewhLKojuQd-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_videos_from_path(\"./videos-dynaq\", speed=0.5)"
      ],
      "metadata": {
        "id": "eH5qU0mrQflJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dyna-Q com Prioritized Sweeping"
      ],
      "metadata": {
        "id": "joZ-esHO4TS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - **Atualizações Prioritárias**: Prioritized Sweeping foca nos estados e ações que mais precisam de atualização, baseando-se em um critério de prioridade como o erro de predição. Isso torna o algoritmo mais eficiente porque direciona o esforço computacional para onde ele é mais necessário.\n",
        " - **Convergência Rápida**: Ao realizar atualizações em estados e ações com grandes diferenças de predição, o aprendizado é acelerado. O agente ajusta rapidamente suas políticas e valores de estado para refletir as partes mais críticas do espaço de estados.\n",
        " - **Eficiência em Grandes Espaços**: Em ambientes com grandes estados de espaço, Prioritized Sweeping permite que o agente ignore áreas menos relevantes, reduzindo significativamente o tempo necessário para alcançar convergência.\n",
        " - **Adaptabilidade**: A abordagem de priorização permite que o agente se adapte mais prontamente a mudanças no ambiente ou na dinâmica do modelo, fazendo uso produtivo das informações recentes mais impactantes."
      ],
      "metadata": {
        "id": "ylPGoiq04yKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "\n",
        "ROWS = 6\n",
        "COLS = 9\n",
        "\n",
        "S = (2, 0)\n",
        "G = (0, 8)\n",
        "\n",
        "BLOCKS = [(1, 2), (2, 2), (3, 2), (0, 7), (1, 7), (2, 7), (4, 5)]\n",
        "\n",
        "ACTIONS = [\"left\", \"up\", \"right\", \"down\"]"
      ],
      "metadata": {
        "id": "GuwPvV8t4_oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Maze:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.rows = ROWS\n",
        "        self.cols = COLS\n",
        "        self.start = S\n",
        "        self.goal = G\n",
        "        self.blocks = BLOCKS\n",
        "        self.state = S\n",
        "        self.end = False\n",
        "\n",
        "        # init maze\n",
        "        self.maze = np.zeros((self.rows, self.cols))\n",
        "        for b in self.blocks:\n",
        "            self.maze[b] = -1\n",
        "\n",
        "    def nxtPosition(self, action):\n",
        "        r, c = self.state\n",
        "        if action == \"left\":\n",
        "            c -= 1\n",
        "        elif action == \"right\":\n",
        "            c += 1\n",
        "        elif action == \"up\":\n",
        "            r -= 1\n",
        "        else:\n",
        "            r += 1\n",
        "\n",
        "        if (r >= 0 and r <= self.rows - 1) and (c >= 0 and c <= self.cols - 1):\n",
        "            if (r, c) not in self.blocks:\n",
        "                self.state = (r, c)\n",
        "        return self.state\n",
        "\n",
        "    def giveReward(self):\n",
        "        if self.state == self.goal:\n",
        "            self.end = True\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def showMaze(self):\n",
        "        self.maze[self.state] = 1\n",
        "        for i in range(0, self.rows):\n",
        "            print('-------------------------------------')\n",
        "            out = '| '\n",
        "            for j in range(0, self.cols):\n",
        "                if self.maze[i, j] == 1:\n",
        "                    token = '*'\n",
        "                if self.maze[i, j] == -1:\n",
        "                    token = 'z'\n",
        "                if self.maze[i, j] == 0:\n",
        "                    token = '0'\n",
        "                out += token + ' | '\n",
        "            print(out)\n",
        "        print('-------------------------------------')"
      ],
      "metadata": {
        "id": "sFUxPULD5JV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PriorityAgent:\n",
        "\n",
        "    def __init__(self, exp_rate=0.3, lr=0.1, n_steps=5, episodes=10, theta=0):\n",
        "        self.maze = Maze()\n",
        "        self.state = S\n",
        "        self.actions = ACTIONS\n",
        "        self.state_actions = []  # state & action track\n",
        "        self.exp_rate = exp_rate\n",
        "        self.lr = lr\n",
        "\n",
        "        self.steps = n_steps\n",
        "        self.episodes = episodes  # number of episodes going to play\n",
        "        self.steps_per_episode = []\n",
        "\n",
        "        self.Q_values = {}\n",
        "        # model function\n",
        "        self.model = {}\n",
        "        for row in range(ROWS):\n",
        "            for col in range(COLS):\n",
        "                self.Q_values[(row, col)] = {}\n",
        "                for a in self.actions:\n",
        "                    self.Q_values[(row, col)][a] = 0\n",
        "\n",
        "        # for priority sweeping\n",
        "        self.theta = theta\n",
        "        self.queue = PriorityQueue()\n",
        "        self.predecessors = {}  # nxtState -> list[(curState, Action)...]\n",
        "\n",
        "    def chooseAction(self):\n",
        "        # epsilon-greedy\n",
        "        mx_nxt_reward = -999\n",
        "        action = \"\"\n",
        "\n",
        "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
        "            action = np.random.choice(self.actions)\n",
        "        else:\n",
        "            # greedy action\n",
        "            current_position = self.state\n",
        "            # if all actions have same value, then select randomly\n",
        "            if len(set(self.Q_values[current_position].values())) == 1:\n",
        "                action = np.random.choice(self.actions)\n",
        "            else:\n",
        "                for a in self.actions:\n",
        "                    nxt_reward = self.Q_values[current_position][a]\n",
        "                    if nxt_reward >= mx_nxt_reward:\n",
        "                        action = a\n",
        "                        mx_nxt_reward = nxt_reward\n",
        "        return action\n",
        "\n",
        "    def reset(self):\n",
        "        self.maze = Maze()\n",
        "        self.state = S\n",
        "        self.state_actions = []\n",
        "\n",
        "    def play(self):\n",
        "        for ep in range(self.episodes):\n",
        "            while not self.maze.end:\n",
        "\n",
        "                action = self.chooseAction()\n",
        "                self.state_actions.append((self.state, action))\n",
        "\n",
        "                nxtState = self.maze.nxtPosition(action)\n",
        "                reward = self.maze.giveReward()\n",
        "\n",
        "                # update priority queue\n",
        "                tmp_diff = reward + np.max(list(self.Q_values[nxtState].values())) - self.Q_values[self.state][action]\n",
        "                if tmp_diff > self.theta:\n",
        "                    self.queue.put((-tmp_diff, (self.state, action)))  # -diff -> (state, action) pop the smallest\n",
        "\n",
        "                # update model & predecessors\n",
        "                if self.state not in self.model.keys():\n",
        "                    self.model[self.state] = {}\n",
        "                self.model[self.state][action] = (reward, nxtState)\n",
        "                if nxtState not in self.predecessors.keys():\n",
        "                    self.predecessors[nxtState] = [(self.state, action)]\n",
        "                else:\n",
        "                    self.predecessors[nxtState].append((self.state, action))\n",
        "                self.state = nxtState\n",
        "\n",
        "                # loop n times to randomly update Q-value\n",
        "                for _ in range(self.steps):\n",
        "                    if self.queue.empty():\n",
        "                        break\n",
        "                    _state, _action = self.queue.get()[1]\n",
        "                    _reward, _nxtState = self.model[_state][_action]\n",
        "                    self.Q_values[_state][_action] += self.lr * (_reward + np.max(list(self.Q_values[_nxtState].values())) - self.Q_values[_state][_action])\n",
        "\n",
        "                    # loop for all state, action predicted lead to _state\n",
        "                    if _state not in self.predecessors.keys():\n",
        "                        continue\n",
        "                    pre_state_action_list = self.predecessors[_state]\n",
        "\n",
        "                    for (pre_state, pre_action) in pre_state_action_list:\n",
        "                        pre_reward, _ = self.model[pre_state][pre_action]\n",
        "                        pre_tmp_diff = pre_reward + np.max(list(self.Q_values[_state].values())) - self.Q_values[pre_state][pre_action]\n",
        "                        if pre_tmp_diff > self.theta:\n",
        "                            self.queue.put((-pre_tmp_diff, (pre_state, pre_action)))\n",
        "            # end of game\n",
        "            if ep % 10 == 0:\n",
        "                print(\"episode\", ep)\n",
        "            self.steps_per_episode.append(len(self.state_actions))\n",
        "            self.reset()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pa = PriorityAgent()\n",
        "    pa.play()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO3NT4OrVTqh",
        "outputId": "55e9b735-1364-4a84-8234-c3b1d8e17810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "ROWS = 6\n",
        "COLS = 9\n",
        "\n",
        "S = (2, 0)\n",
        "G = (0, 8)\n",
        "\n",
        "BLOCKS = [(1, 2), (2, 2), (3, 2), (0, 7), (1, 7), (2, 7), (4, 5)]\n",
        "\n",
        "ACTIONS = [\"left\", \"up\", \"right\", \"down\"]"
      ],
      "metadata": {
        "id": "LQGhSBvE0CAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ambiente de labirinto\n",
        "class Maze:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.rows = ROWS\n",
        "        self.cols = COLS\n",
        "        self.start = S\n",
        "        self.goal = G\n",
        "        self.blocks = BLOCKS\n",
        "        self.state = S\n",
        "        self.end = False\n",
        "        # init maze\n",
        "        self.maze = np.zeros((self.rows, self.cols))\n",
        "        for b in self.blocks:\n",
        "            self.maze[b] = -1\n",
        "\n",
        "    def nxtPosition(self, action):\n",
        "        r, c = self.state\n",
        "        if action == \"left\":\n",
        "            c -= 1\n",
        "        elif action == \"right\":\n",
        "            c += 1\n",
        "        elif action == \"up\":\n",
        "            r -= 1\n",
        "        else:\n",
        "            r += 1\n",
        "\n",
        "        if (r >= 0 and r <= self.rows-1) and (c >= 0 and c <= self.cols-1):\n",
        "            if (r, c) not in self.blocks:\n",
        "                self.state = (r, c)\n",
        "        return self.state\n",
        "\n",
        "    def giveReward(self):\n",
        "        if self.state == self.goal:\n",
        "            self.end = True\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def showMaze(self):\n",
        "        self.maze[self.state] = 1\n",
        "        for i in range(0, self.rows):\n",
        "            print('-------------------------------------')\n",
        "            out = '| '\n",
        "            for j in range(0, self.cols):\n",
        "                if self.maze[i, j] == 1:\n",
        "                    token = '*'\n",
        "                if self.maze[i, j] == -1:\n",
        "                    token = 'z'\n",
        "                if self.maze[i, j] == 0:\n",
        "                    token = '0'\n",
        "                out += token + ' | '\n",
        "            print(out)\n",
        "        print('-------------------------------------')"
      ],
      "metadata": {
        "id": "05ybdhFR0EfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PriorityAgent:\n",
        "\n",
        "    def __init__(self, exp_rate=0.3, lr=0.1, n_steps=5, episodes=1, theta=0):\n",
        "        self.maze = Maze()\n",
        "        self.state = S\n",
        "        self.actions = ACTIONS\n",
        "        self.state_actions = []  # state & action track\n",
        "        self.exp_rate = exp_rate\n",
        "        self.lr = lr\n",
        "\n",
        "        self.steps = n_steps\n",
        "        self.episodes = episodes  # number of episodes going to play\n",
        "        self.steps_per_episode = []\n",
        "\n",
        "        self.Q_values = {}\n",
        "        # model function\n",
        "        self.model = {}\n",
        "        for row in range(ROWS):\n",
        "            for col in range(COLS):\n",
        "                self.Q_values[(row, col)] = {}\n",
        "                for a in self.actions:\n",
        "                    self.Q_values[(row, col)][a] = 0\n",
        "\n",
        "        # for priority sweeping\n",
        "        self.theta = theta\n",
        "        self.queue = PriorityQueue()\n",
        "        self.predecessors = {}  # nxtState -> list[(curState, Action)...]\n",
        "\n",
        "    def chooseAction(self):\n",
        "        # epsilon-greedy\n",
        "        mx_nxt_reward = -999\n",
        "        action = \"\"\n",
        "\n",
        "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
        "            action = np.random.choice(self.actions)\n",
        "        else:\n",
        "            # greedy action\n",
        "            current_position = self.state\n",
        "            # if all actions have same value, then select randomly\n",
        "            if len(set(self.Q_values[current_position].values())) == 1:\n",
        "                action = np.random.choice(self.actions)\n",
        "            else:\n",
        "                for a in self.actions:\n",
        "                    nxt_reward = self.Q_values[current_position][a]\n",
        "                    if nxt_reward >= mx_nxt_reward:\n",
        "                        action = a\n",
        "                        mx_nxt_reward = nxt_reward\n",
        "        return action\n",
        "\n",
        "    def reset(self):\n",
        "        self.maze = Maze()\n",
        "        self.state = S\n",
        "        self.state_actions = []\n",
        "\n",
        "    def play(self):\n",
        "        for ep in range(self.episodes):\n",
        "            while not self.maze.end:\n",
        "\n",
        "                action = self.chooseAction()\n",
        "                self.state_actions.append((self.state, action))\n",
        "\n",
        "                nxtState = self.maze.nxtPosition(action)\n",
        "                reward = self.maze.giveReward() #fazer lista de recompensas\n",
        "\n",
        "                # atualiza a fila  de prioridades\n",
        "                tmp_diff = reward + np.max(list(self.Q_values[nxtState].values())) - self.Q_values[self.state][action]\n",
        "                if tmp_diff > self.theta:\n",
        "                    self.queue.put((-tmp_diff, (self.state, action)))  # -diff -> (state, action) pop the smallest\n",
        "\n",
        "                # update model & predecessors\n",
        "                if self.state not in self.model.keys():\n",
        "                    self.model[self.state] = {}\n",
        "                self.model[self.state][action] = (reward, nxtState)\n",
        "                if nxtState not in self.predecessors.keys():\n",
        "                    self.predecessors[nxtState] = [(self.state, action)]\n",
        "                else:\n",
        "                    self.predecessors[nxtState].append((self.state, action))\n",
        "                self.state = nxtState\n",
        "\n",
        "                # loop n times to randomly update Q-value\n",
        "                for _ in range(self.steps):\n",
        "                    if self.queue.empty():\n",
        "                        break\n",
        "                    _state, _action = self.queue.get()[1]\n",
        "                    _reward, _nxtState = self.model[_state][_action]\n",
        "                    self.Q_values[_state][_action] += self.lr * (_reward + np.max(list(self.Q_values[_nxtState].values())) - self.Q_values[_state][_action])\n",
        "\n",
        "                    # loop for all state, action predicted lead to _state\n",
        "                    if _state not in self.predecessors.keys():\n",
        "                        continue\n",
        "                    pre_state_action_list = self.predecessors[_state]\n",
        "\n",
        "                    for (pre_state, pre_action) in pre_state_action_list:\n",
        "                        pre_reward, _ = self.model[pre_state][pre_action]\n",
        "                        pre_tmp_diff = pre_reward + np.max(list(self.Q_values[_state].values())) - self.Q_values[pre_state][pre_action]\n",
        "                        if pre_tmp_diff > self.theta:\n",
        "                            self.queue.put((-pre_tmp_diff, (pre_state, pre_action)))\n",
        "            # end of game\n",
        "            if ep % 10 == 0:\n",
        "                print(\"episode\", ep)\n",
        "            self.steps_per_episode.append(len(self.state_actions))\n",
        "            self.reset()"
      ],
      "metadata": {
        "id": "5cIGBEmx0IA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPISODES = 50"
      ],
      "metadata": {
        "id": "3GBENoX00KQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparação"
      ],
      "metadata": {
        "id": "EUsSmOtF0OEG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UdfpakqZkaxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import DynaMaze"
      ],
      "metadata": {
        "id": "5bQSUYNN0NaO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "e41e9bb3-0368-414a-eb0b-e060e010c7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'DynaMaze'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-feea0d71391b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mDynaMaze\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DynaMaze'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = PriorityAgent(n_steps=5, episodes=N_EPISODES)\n",
        "agent.play()\n",
        "\n",
        "steps_episode_pa = agent.steps_per_episode"
      ],
      "metadata": {
        "id": "AiCDmWJx0TiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DynaMaze.DynaAgent(n_steps=5, episodes=N_EPISODES)\n",
        "agent.play()\n",
        "\n",
        "steps_episode_da = agent.steps_per_episode"
      ],
      "metadata": {
        "id": "k4obj42d0VIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=[10, 6])\n",
        "\n",
        "plt.ylim(0, 500)\n",
        "plt.plot(range(N_EPISODES), steps_episode_pa, label=\"Priority Sweeping\")\n",
        "plt.plot(range(N_EPISODES), steps_episode_da, label=\"Dyna-Q\")\n",
        "\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "idNAB5sv0Xn0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}